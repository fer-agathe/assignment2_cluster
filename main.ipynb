{"cells":[{"cell_type":"markdown","metadata":{"id":"uTv0D26B9W2h"},"source":["# Assignment 2"]},{"cell_type":"markdown","metadata":{"id":"IdN5pnC8MMx-"},"source":["This notebook is intended to produce the plots and figures for the report on Problem 1 of the practical. You should not run this notebook in Google Colab until you have finished constructing the correct solutions for transformer_solution.py and encoder_decoder_solution.py\n","\n","This notebook provides some limited commentary on several HuggingFace Features and toolage. You will use HuggingFace Datasets to load the Amazon Polarity dataset for sentiment analysis. The notebook will define a Bert tokenizer, collate functions, and then train and evaluate several models using the HuggingFace utilities mentioned above. Remember, the most crucial part here is running the experiments for the report."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1710783128925,"user":{"displayName":"Agathe FERNANDES MACHADO","userId":"16333650957587640022"},"user_tz":240},"id":"4ftvFaaiSBvU"},"outputs":[],"source":["%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710780806255,"user":{"displayName":"Agathe FERNANDES MACHADO","userId":"16333650957587640022"},"user_tz":240},"id":"5M0JjnrOSBvV"},"outputs":[],"source":["# allows plots to be displayed directly in the notebook rather than in a separate window\n","#  loads the autoreload extension : enables automatic reloading of modules before executing code\n","# if there are changes in a module that has been imported, those changes will be automatically reflected without needing to restart the kernel\n","# set up of automatic reloading, more aggressive approach than %autoreload 1\n","# autoreload 2 reload all modules except those excluded by being explicitly marked to be ignored or excluded"]},{"cell_type":"markdown","metadata":{"id":"qO-EY1oSSBvV"},"source":["### Mount your Google Drive"]},{"cell_type":"markdown","metadata":{"id":"C3k-Mei1SBvV"},"source":["### Link your assignment folder & install requirements\n","Enter the path to the assignment folder in your Google Drive\n","If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n","you can delete this cell which is specific to Google Colab."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":608,"status":"ok","timestamp":1710784371032,"user":{"displayName":"Agathe FERNANDES MACHADO","userId":"16333650957587640022"},"user_tz":240},"id":"oODLwt1QzgGa"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/s0/9my1yr7920lfy9vmb78bvjch0000gn/T/ipykernel_10858/2122472430.py:19: UserWarning: CUDA is not available.\n","  warnings.warn('CUDA is not available.')\n"]}],"source":["import sys\n","import os\n","import shutil\n","import warnings\n","import json\n","\n","folder = \"\" #@param {type:\"string\"}\n","#!ln -Ts \"$folder\" /content/deepL_assignment2> /dev/null\n","#!cp gdrive/MyDrive/deepL_assignment2/transformer.py .\n","#!cp gdrive/MyDrive/deepL_assignment2/lstm.py .\n","\n","# Add the assignment folder to Python path\n","if '/content/assignment' not in sys.path:\n","  sys.path.insert(0, '/content/assignment')\n","\n","# Check if CUDA is available\n","import torch\n","if not torch.cuda.is_available():\n","  warnings.warn('CUDA is not available.')"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"dt3NTvpsy4Oc"},"source":["### Running on GPU\n","For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n","* (EN) `Edit > Notebook Settings`\n","* (FR) `Modifier > ParamÃ¨tres du notebook`\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1345,"status":"ok","timestamp":1710784372375,"user":{"displayName":"Agathe FERNANDES MACHADO","userId":"16333650957587640022"},"user_tz":240},"id":"RLVSmv9HoMH5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/agathefernandesmachado/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import urllib.request\n","from sklearn.metrics import f1_score, accuracy_score\n","import time\n","\n","from typing import List, Dict, Union, Optional, Tuple\n","import torch\n","\n","from dataclasses import dataclass\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","from tqdm.auto import tqdm\n","\n","\n","from datasets import Dataset\n","import transformers\n","\n","from datasets import load_dataset\n","from tokenizers import Tokenizer\n","\n","from transformer import Transformer, MultiHeadedAttention\n","from lstm import EncoderDecoder"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11661,"status":"ok","timestamp":1710784384034,"user":{"displayName":"Agathe FERNANDES MACHADO","userId":"16333650957587640022"},"user_tz":240},"id":"R_ACGnKSSBvX","outputId":"753da673-3f6f-40cd-e170-c87481116139"},"outputs":[],"source":["dataset_train = load_dataset(\"yelp_polarity\", split=\"train\", cache_dir=\"assignment/data\")\n","dataset_test = load_dataset(\"yelp_polarity\", split=\"test[:1000]\", cache_dir=\"assignment/data\")"]},{"cell_type":"markdown","metadata":{"id":"mC_VqtTsSBvY"},"source":["### ðŸ” Quick look at the data\n","Lets have quick look at a few samples in our test set."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710784385279,"user":{"displayName":"Agathe FERNANDES MACHADO","userId":"16333650957587640022"},"user_tz":240},"id":"EcKSBoLYSBvY","outputId":"128da602-8fd4-4c12-c54d-54d9f897b9c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["------------------------------\n","title: Contrary to other reviews, I have zero complaints about the service or the prices. I have been getting tire service here for the past 5 years now, and compared to my experience with places like Pep Boys, these guys are experienced and know what they're doing. \\nAlso, this is one place that I do not feel like I am being taken advantage of, just because of my gender. Other auto mechanics have been notorious for capitalizing on my ignorance of cars, and have sucked my bank account dry. But here, my service and road coverage has all been well explained - and let up to me to decide. \\nAnd they just renovated the waiting room. It looks a lot better than it did in previous years.\n","label: 1\n","------------------------------\n","title: Last summer I had an appointment to get new tires and had to wait a super long time. I also went in this week for them to fix a minor problem with a tire they put on. They \\\"\"fixed\\\"\" it for free, and the very next morning I had the same issue. I called to complain, and the \\\"\"manager\\\"\" didn't even apologize!!! So frustrated. Never going back.  They seem overpriced, too.\n","label: 0\n","------------------------------\n","title: Friendly staff, same starbucks fair you get anywhere else.  Sometimes the lines can get long.\n","label: 1\n"]}],"source":["n_samples_to_see = 3\n","for i in range(n_samples_to_see):\n","  print(\"-\"*30)\n","  print(\"title:\", dataset_test[i][\"text\"])\n","  print(\"label:\", dataset_test[i][\"label\"])"]},{"cell_type":"markdown","metadata":{"id":"fzxwRDQFUtaG"},"source":["### 1ï¸. Tokenize the `text`\n","Tokenize the `text`portion of each sample (i.e. parsing the text to smaller chunks). Tokenization can happen in many ways; traditionally, this was done based on the white spaces. With transformer-based models, tokenization is performed based on the frequency of occurrence of \"chunk of text\". This frequency can be learned in many different ways. However the most common one is the [**wordpiece**](https://arxiv.org/pdf/1609.08144v2.pdf) model.\n","> The wordpiece model is generated using a data-driven approach to maximize the language-model likelihood\n","of the training data, given an evolving word definition. Given a training corpus and a number of desired\n","tokens $D$, the optimization problem is to select $D$ wordpieces such that the resulting corpus is minimal in the\n","number of wordpieces when segmented according to the chosen wordpiece model.\n","\n","Under this model:\n","1. Not all things can be converted to tokens depending on the model. For example, most models have been pretrained without any knowledge of emojis. So their token will be `[UNK]`, which stands for unknown.\n","2. Some words will be mapped to multiple tokens!\n","3. Depending on the kind of model, your tokens may or may not respect capitalization"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1395,"status":"ok","timestamp":1710784390669,"user":{"displayName":"Agathe FERNANDES MACHADO","userId":"16333650957587640022"},"user_tz":240},"id":"qCpNwaTYSo3U"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1710784390669,"user":{"displayName":"Agathe FERNANDES MACHADO","userId":"16333650957587640022"},"user_tz":240},"id":"3OMDqabyToBt","outputId":"af13049c-8778-4d1f-8f85-1e284527c3b8"},"outputs":[{"data":{"text/plain":["['welcome',\n"," 'to',\n"," 'if',\n"," '##t',\n"," '##6',\n"," '##13',\n"," '##5',\n"," '.',\n"," 'we',\n"," 'now',\n"," 'teach',\n"," 'you',\n"," '[UNK]',\n"," '(',\n"," 'hugging',\n"," 'face',\n"," ')',\n"," 'library',\n"," ':',\n"," 'dd',\n"," '##d',\n"," '.']"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["input_sample = \"Welcome to IFT6135. We now teach you ðŸ¤—(HUGGING FACE) Library :DDD.\"\n","tokenizer.tokenize(input_sample)"]},{"cell_type":"markdown","metadata":{"id":"NEu6aqReXqp6"},"source":["### 2. Encoding\n","Once we have tokenized the text, we then need to convert these chuncks to numbers so we can feed them to our model. This conversion is basically a look-up in a dictionary **from `str` $\\to$ `int`**. The tokenizer object can also perform this work. While it does so it will also add the *special* tokens needed by the model to the encodings."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3730,"status":"ok","timestamp":1710784396777,"user":{"displayName":"Agathe FERNANDES MACHADO","userId":"16333650957587640022"},"user_tz":240},"id":"EpDGccrvYKnT","outputId":"63333d12-12eb-4e97-9a41-5490837af73b"},"outputs":[{"name":"stdout","output_type":"stream","text":["--> Token Encodings:\n"," [101, 6160, 2000, 2065, 2102, 2575, 17134, 2629, 1012, 2057, 2085, 6570, 2017, 100, 1006, 17662, 2227, 1007, 3075, 1024, 20315, 2094, 1012, 102]\n","-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\n","--> Token Encodings Decoded:\n"," [CLS] welcome to ift6135. we now teach you [UNK] ( hugging face ) library : ddd. [SEP]\n"]}],"source":["input_sample = \"Welcome to IFT6135. We now teach you ðŸ¤—(HUGGING FACE) Library :DDD.\" #@param {type: \"string\"}\n","\n","print(\"--> Token Encodings:\\n\",tokenizer.encode(input_sample))\n","print(\"-.\"*15)\n","print(\"--> Token Encodings Decoded:\\n\",tokenizer.decode(tokenizer.encode(input_sample)))"]},{"cell_type":"markdown","metadata":{"id":"DI8lFKZSZ2ZW"},"source":["### 3ï¸. Truncate/Pad samples\n","Since all the sample in the batch will not have the same sequence length, we would need to truncate the longer sequences (i.e. the ones that exeed a predefined maximum length) and pad the shorter ones so we that we can equal length for all the samples in the batch. Once this is achieved, we would need to convert the result to `torch.Tensor`s and return. These tensors will then be retrieved from the [dataloader](https://https//pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1249,"status":"ok","timestamp":1710784398022,"user":{"displayName":"Agathe FERNANDES MACHADO","userId":"16333650957587640022"},"user_tz":240},"id":"OizjCxIYSBvZ"},"outputs":[],"source":["class Collate:\n","    def __init__(self, tokenizer: str, max_len: int) -> None:\n","        self.tokenizer_name = tokenizer\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n","        self.max_len = max_len\n","\n","    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n","        texts = list(map(lambda batch_instance: batch_instance[\"text\"], batch))\n","        tokenized_inputs = self.tokenizer(\n","            texts,\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=self.max_len,\n","            return_tensors=\"pt\",\n","            return_token_type_ids=False,\n","        )\n","\n","        labels = list(map(lambda batch_instance: int(batch_instance[\"label\"]), batch))\n","        labels = torch.LongTensor(labels)\n","        return dict(tokenized_inputs, **{\"labels\": labels})"]},{"cell_type":"markdown","metadata":{"id":"vmTM18b0SBvZ"},"source":["#### ðŸ§‘â€ðŸ³ Setting up the collate function"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710784400358,"user":{"displayName":"Agathe FERNANDES MACHADO","userId":"16333650957587640022"},"user_tz":240},"id":"4VaSpuyIjNqn"},"outputs":[],"source":["tokenizer_name = \"bert-base-uncased\"\n","sample_max_length = 256\n","collate = Collate(tokenizer=tokenizer_name, max_len=sample_max_length)"]},{"cell_type":"markdown","metadata":{"id":"CSJm3k4ASBva"},"source":["### 4. Models"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2347,"status":"ok","timestamp":1710784402703,"user":{"displayName":"Agathe FERNANDES MACHADO","userId":"16333650957587640022"},"user_tz":240},"id":"y9P4oWyOSexA"},"outputs":[],"source":["from transformers import AutoModel\n","import torch.nn as nn\n","torch.random.manual_seed(0)\n","\n","class ReviewClassifier(nn.Module):\n","    def __init__(self, backbone: str, backbone_hidden_size: int, nb_classes: int):\n","        super(ReviewClassifier, self).__init__()\n","        self.backbone = backbone\n","        self.backbone_hidden_size = backbone_hidden_size\n","        self.nb_classes = nb_classes\n","        self.back_bone = AutoModel.from_pretrained(\n","            self.backbone,\n","            output_attentions=False,\n","            output_hidden_states=False,\n","        )\n","        self.classifier = torch.nn.Linear(self.backbone_hidden_size, self.nb_classes)\n","\n","    def forward(\n","        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n","    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n","        back_bone_output = self.back_bone(input_ids, attention_mask=attention_mask)\n","        hidden_states = back_bone_output[0]\n","        pooled_output = hidden_states[:, 0]  # getting the [CLS] token\n","        logits = self.classifier(pooled_output)\n","        if labels is not None:\n","            loss_fn = torch.nn.CrossEntropyLoss()\n","            loss = loss_fn(\n","                logits.view(-1, self.nb_classes),\n","                labels.view(-1),\n","            )\n","            return loss, logits\n","        return logits\n","\n","class ReviewClassifierLSTM(nn.Module):\n","    def __init__(self, nb_classes: int, encoder_only: bool = False,\n","        with_attn: bool = True, dropout: int = 0.5, hidden_size: int = 256):\n","        super(ReviewClassifierLSTM, self).__init__()\n","        self.nb_classes = nb_classes\n","        self.encoder_only = encoder_only\n","\n","        if with_attn:\n","            attn = MultiHeadedAttention(head_size = 2*hidden_size, num_heads=1)\n","        else:\n","            attn = None\n","\n","        self.back_bone = EncoderDecoder(dropout=dropout, encoder_only=encoder_only,\n","                                        attn=attn, hidden_size=hidden_size)\n","\n","        if self.encoder_only:\n","            self.classifier = torch.nn.Linear(hidden_size*2, self.nb_classes)\n","        else:\n","            self.classifier = torch.nn.Linear(hidden_size, self.nb_classes)\n","\n","    def forward(\n","        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n","    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n","        pooled_output, _ = self.back_bone(input_ids, attention_mask)\n","        logits = self.classifier(pooled_output)\n","        if labels is not None:\n","            loss_fn = torch.nn.CrossEntropyLoss()\n","            loss = loss_fn(\n","                logits.view(-1, self.nb_classes),\n","                labels.view(-1),\n","            )\n","            return loss, logits\n","        return logits\n","\n","\n","class ReviewClassifierTransformer(nn.Module):\n","    def __init__(self, nb_classes: int, num_heads: int = 4, num_layers: int = 4, block: str=\"prenorm\", dropout: float = 0.3):\n","        super(ReviewClassifierTransformer, self).__init__()\n","        self.nb_classes = nb_classes\n","        self.back_bone = Transformer(num_heads=num_heads, num_layers=num_layers, block=block, dropout=dropout)\n","        self.classifier = torch.nn.Linear(256, self.nb_classes)\n","\n","    def forward(\n","        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n","    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n","        attention_mask = torch.cat([torch.ones(attention_mask.shape[0]).unsqueeze(1).to(device),\n","                                    attention_mask], dim=1)\n","        back_bone_output = self.back_bone(input_ids, attention_mask)\n","        hidden_states = back_bone_output\n","        pooled_output = hidden_states\n","        logits = self.classifier(pooled_output)\n","        if labels is not None:\n","            loss_fn = torch.nn.CrossEntropyLoss()\n","            loss = loss_fn(\n","                logits.view(-1, self.nb_classes),\n","                labels.view(-1),\n","            )\n","            return loss, logits\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"WbgoiBkhSBva"},"source":["### 5. Trainer"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1710784404611,"user":{"displayName":"Agathe FERNANDES MACHADO","userId":"16333650957587640022"},"user_tz":240},"id":"HP58LrWUjFt4","outputId":"7a42e189-b408-444f-a447-886f0a8ab361"},"outputs":[{"name":"stdout","output_type":"stream","text":["--> Device selected: cpu\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","print(f\"--> Device selected: {device}\")\n","def train_one_epoch(\n","    model: torch.nn.Module, training_data_loader: DataLoader, optimizer: torch.optim.Optimizer, logging_frequency: int, testing_data_loader: DataLoader, logger: dict):\n","    model.train()\n","    optimizer.zero_grad()\n","    epoch_loss = 0\n","    logging_loss = 0\n","    start_time = time.time()\n","    mini_start_time = time.time()\n","    for step, batch in enumerate(training_data_loader):\n","        batch = {key: value.to(device) for key, value in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs[0]\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        logging_loss += loss.item()\n","\n","        if (step + 1) % logging_frequency == 0:\n","            freq_time = time.time()-mini_start_time\n","            logger['train_time'].append(freq_time+logger['train_time'][-1])\n","            logger['train_losses'].append(logging_loss/logging_frequency)\n","            logger['train_torch.cuda.memory_allocated'] = torch.cuda.memory_allocated(0)/1024/1024/1024 # GB\n","            logger['train_torch.cuda.memory_reserved'] = torch.cuda.memory_reserved(0)/1024/1024/1024 # GB\n","            logger['train_torch.cuda.max_memory_reserved'] = torch.cuda.max_memory_reserved(0)/1024/1024/1024 # GB\n","            print(f\"Training loss @ step {step+1}: {logging_loss/logging_frequency}\")\n","            eval_acc, eval_f1, eval_loss, eval_time = evaluate(model, testing_data_loader)\n","            logger['eval_accs'].append(eval_acc)\n","            logger['eval_f1s'].append(eval_f1)\n","            logger['eval_losses'].append(eval_loss)\n","            logger['eval_time'].append(eval_time+logger['eval_time'][-1])\n","            logger['eval_torch.cuda.memory_allocated'] = torch.cuda.memory_allocated(0)/1024/1024/1024 # GB\n","            logger['eval_torch.cuda.memory_reserved'] = torch.cuda.memory_reserved(0)/1024/1024/1024 # GB\n","            logger['eval_torch.cuda.max_memory_reserved'] = torch.cuda.max_memory_reserved(0)/1024/1024/1024 # GB\n","            logging_loss = 0\n","            mini_start_time = time.time()\n","    return epoch_loss / len(training_data_loader), time.time()-start_time\n","\n","\n","def evaluate(model: torch.nn.Module, test_data_loader: DataLoader):\n","    model.eval()\n","    model.to(device)\n","    eval_loss = 0\n","    correct_predictions = {i: 0 for i in range(2)}\n","    total_predictions = {i: 0 for i in range(2)}\n","    preds = []\n","    targets = []\n","    start_time = time.time()\n","    with torch.no_grad():\n","        for step, batch in enumerate(test_data_loader):\n","            batch = {key: value.to(device) for key, value in batch.items()}\n","            outputs = model(**batch)\n","            loss = outputs[0]\n","            eval_loss += loss.item()\n","\n","            predictions = np.argmax(outputs[1].detach().cpu().numpy(), axis=1)\n","            preds.extend(predictions.tolist())\n","            targets.extend(batch[\"labels\"].cpu().numpy().tolist())\n","\n","            for target, prediction in zip(batch[\"labels\"].cpu().numpy(), predictions):\n","                if target == prediction:\n","                    correct_predictions[target] += 1\n","                total_predictions[target] += 1\n","    accuracy = (100.0 * sum(correct_predictions.values())) / sum(total_predictions.values())\n","    f1 = f1_score(targets, preds)\n","    model.train()\n","    return accuracy, round(f1, 4), eval_loss / len(test_data_loader), time.time() - start_time\n","\n","\n","def save_logs(dictionary, log_dir, exp_id):\n","  log_dir = os.path.join(log_dir, exp_id)\n","  os.makedirs(log_dir, exist_ok=True)\n","  # Log arguments\n","  with open(os.path.join(log_dir, \"args.json\"), \"w\") as f:\n","    json.dump(dictionary, f, indent=2)\n","\n","def save_model(model, log_dir, exp_id):\n","  log_dir = os.path.join(log_dir, exp_id)\n","  os.makedirs(log_dir, exist_ok=True)\n","  # Save model\n","  torch.save(model.state_dict(), f\"assignment/models/model_{exp_id}.pt\")\n","\n","batch_size = 512\n","\n","train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate)\n","test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate)"]},{"cell_type":"markdown","metadata":{"id":"m3g8zT2oSBva"},"source":["### 6. Problem 3\n","Feel free to modify this code however it is convenient for you to produce a report except for the model parameters."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ys7Z3VfGSBvb","outputId":"538ad014-95b6-45ac-841e-b1b317f8a4f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Setting 1: LSTM, no dropout, encoder only\n","Epoch 1\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder.layer.11\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     66\u001b[0m         param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m train_loss, train_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain, torch.cuda.memory_allocated: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124mGB\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain, torch.cuda.memory_reserved: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124mGB\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_reserved(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m))\n","Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, training_data_loader, optimizer, logging_frequency, testing_data_loader, logger)\u001b[0m\n\u001b[1;32m     10\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m mini_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 12\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining_data_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2814\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2814\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2815\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m   2816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2810\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2794\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2792\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2793\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2794\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2795\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2796\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2797\u001b[0m )\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:586\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 586\u001b[0m     pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43m_query_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m     pa_subtable \u001b[38;5;241m=\u001b[39m _query_table_with_indices_mapping(table, key, indices\u001b[38;5;241m=\u001b[39mindices)\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:100\u001b[0m, in \u001b[0;36m_query_table\u001b[0;34m(table, key)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39mtable\u001b[38;5;241m.\u001b[39mslice(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# don't use pyarrow.Table.take even for pyarrow >=1.0 (see https://issues.apache.org/jira/browse/ARROW-9773)\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_rows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m _raise_bad_key_type(key)\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/datasets/table.py:124\u001b[0m, in \u001b[0;36mIndexedTableMixin.fast_gather\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndices must be non-empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msearchsorted(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offsets, indices, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches(\n\u001b[1;32m    123\u001b[0m     [\n\u001b[0;32m--> 124\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_offsets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_indices, indices)\n\u001b[1;32m    126\u001b[0m     ],\n\u001b[1;32m    127\u001b[0m     schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema,\n\u001b[1;32m    128\u001b[0m )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["logging_frequency = 100\n","learning_rate = 1e-5\n","nb_epoch=5\n","\n","for i in range(1, 2):\n","  experimental_setting = i\n","\n","  if experimental_setting == 1:\n","    print(\"Setting 1: LSTM, no dropout, encoder only\")\n","    model = ReviewClassifierLSTM(nb_classes=2, dropout=0, encoder_only=True)\n","  if experimental_setting == 2:\n","    print(\"Setting 2: LSTM, dropout, encoder only\")\n","    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=True)\n","  if experimental_setting == 3:\n","    print(\"Setting 3: LSTM, dropout, encoder-decoder, no attention\")\n","    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=False, with_attn=False)\n","  if experimental_setting == 4:\n","    print(\"Setting 4: LSTM, dropout, encoder-decoder, with attention\")\n","    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=False, with_attn=True)\n","  if experimental_setting == 5:\n","    print(\"Setting 5: Transformer, 2 layers, pre-normalization\")\n","    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='prenorm', dropout=0.3)\n","  if experimental_setting == 6:\n","    print(\"Setting 6: Transformer, 4 layers, pre-normalization\")\n","    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=4, block='prenorm', dropout=0.3)\n","  if experimental_setting == 7:\n","    print(\"Setting 7: Transformer, 2 layers, post-normalization\")\n","    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='postnorm', dropout=0.3)\n","  if experimental_setting == 8:\n","    nb_epoch = 2\n","    print(\"Setting 8: Fine-tuning BERT\")\n","    model = ReviewClassifier(backbone=\"bert-base-uncased\", backbone_hidden_size=768, nb_classes=2)\n","    for parameter in model.back_bone.parameters():\n","      parameter.requires_grad= False\n","\n","\n","  # setting up the optimizer\n","  optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, eps=1e-8)\n","  model.to(device)\n","  logger = dict()\n","  logger['train_time'] = [0]\n","  logger['eval_time'] = [0]\n","  logger['train_losses'] = []\n","  logger['eval_accs'] = []\n","  logger['eval_f1s'] = []\n","  logger['eval_losses'] = []\n","  logger[\"epoch_train_loss\"] = []\n","  logger[\"epoch_train_time\"] = []\n","  logger[\"epoch_eval_loss\"] = []\n","  logger[\"epoch_eval_time\"] = []\n","  logger[\"epoch_eval_acc\"] = []\n","  logger[\"epoch_eval_f1\"] = []\n","  logger['train_torch.cuda.memory_allocated'] = []\n","  logger['train_torch.cuda.memory_reserved'] = []\n","  logger['train_torch.cuda.max_memory_reserved'] = []\n","  logger['eval_torch.cuda.memory_allocated'] = []\n","  logger['eval_torch.cuda.memory_reserved'] = []\n","  logger['eval_torch.cuda.max_memory_reserved'] = []\n","  logger['parameters'] = sum([p.numel() for p in model.back_bone.parameters() if p.requires_grad])\n","\n","  for epoch in range(nb_epoch):\n","    print(f\"Epoch {epoch+1}\")\n","    if experimental_setting == 8 and epoch>1: #unfreezing layer 10 for fine-tuning\n","      for name, param in model.back_bone.named_parameters():\n","        if name.startswith(\"encoder.layer.11\"):\n","            param.requires_grad = True\n","    train_loss, train_time = train_one_epoch(model, train_loader, optimizer, logging_frequency, test_loader, logger)\n","    print(\"Train, torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n","    print(\"Train, torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n","    print(\"Train, torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n","    eval_acc, eval_f1, eval_loss, eval_time  = evaluate(model, test_loader)\n","    print(\"Val, torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n","    print(\"Val, torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n","    print(\"Val, torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n","    logger[\"epoch_train_loss\"].append(train_loss)\n","    logger[\"epoch_train_time\"].append(train_time)\n","    logger[\"epoch_eval_loss\"].append(eval_loss)\n","    logger[\"epoch_eval_time\"].append(eval_time)\n","    logger[\"epoch_eval_acc\"].append(eval_acc)\n","    logger[\"epoch_eval_f1\"].append(eval_f1)\n","    print(f\"    Epoch: {epoch+1} Loss/Test: {eval_loss}, Loss/Train: {train_loss}, Acc/Test: {eval_acc}, F1/Test: {eval_f1}, Train Time: {train_time}, Eval Time: {eval_time}\")\n","\n","  logger['train_time'] = logger['train_time'][1:]\n","  logger['eval_time'] = logger['eval_time'][1:]\n","  save_logs(logger, \"assignment/log\", str(experimental_setting))\n","  save_model(model, \"assignment/models\", str(experimental_setting))"]},{"cell_type":"markdown","metadata":{"id":"VyGj2ushSBvb"},"source":["### 7. Augment the original reviews"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"KleJPzfGSBvb"},"outputs":[{"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/agathefernandesmachado/nltk_data'\n    - '/Users/agathefernandesmachado/Downloads/deepL_assignment2/.venv/nltk_data'\n    - '/Users/agathefernandesmachado/Downloads/deepL_assignment2/.venv/share/nltk_data'\n    - '/Users/agathefernandesmachado/Downloads/deepL_assignment2/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/agathefernandesmachado/nltk_data'\n    - '/Users/agathefernandesmachado/Downloads/deepL_assignment2/.venv/nltk_data'\n    - '/Users/agathefernandesmachado/Downloads/deepL_assignment2/.venv/share/nltk_data'\n    - '/Users/agathefernandesmachado/Downloads/deepL_assignment2/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Augmenter\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordSwapQWERTY\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordSwapExtend\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/textattack/__init__.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     attack_recipes,\n\u001b[1;32m     25\u001b[0m     attack_results,\n\u001b[1;32m     26\u001b[0m     augmentation,\n\u001b[1;32m     27\u001b[0m     commands,\n\u001b[1;32m     28\u001b[0m     constraints,\n\u001b[1;32m     29\u001b[0m     datasets,\n\u001b[1;32m     30\u001b[0m     goal_function_results,\n\u001b[1;32m     31\u001b[0m     goal_functions,\n\u001b[1;32m     32\u001b[0m     loggers,\n\u001b[1;32m     33\u001b[0m     metrics,\n\u001b[1;32m     34\u001b[0m     models,\n\u001b[1;32m     35\u001b[0m     search_methods,\n\u001b[1;32m     36\u001b[0m     shared,\n\u001b[1;32m     37\u001b[0m     transformations,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     41\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextattack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/textattack/augmentation/__init__.py:10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\".. _augmentation:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mTextAttack augmentation package:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mTransformations and constraints can be used outside of an attack for simple NLP data augmentation with the ``Augmenter`` class that returns all possible transformations for a given string.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Augmenter\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecipes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     WordNetAugmenter,\n\u001b[1;32m     12\u001b[0m     EmbeddingAugmenter,\n\u001b[1;32m     13\u001b[0m     CharSwapAugmenter,\n\u001b[1;32m     14\u001b[0m     EasyDataAugmenter,\n\u001b[1;32m     15\u001b[0m     CheckListAugmenter,\n\u001b[1;32m     16\u001b[0m     DeletionAugmenter,\n\u001b[1;32m     17\u001b[0m     CLAREAugmenter,\n\u001b[1;32m     18\u001b[0m     BackTranslationAugmenter,\n\u001b[1;32m     19\u001b[0m )\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/textattack/augmentation/recipes.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstraints\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msemantics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentence_encoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UniversalSentenceEncoder\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Augmenter\n\u001b[0;32m---> 19\u001b[0m DEFAULT_CONSTRAINTS \u001b[38;5;241m=\u001b[39m [RepeatModification(), \u001b[43mStopwordModification\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEasyDataAugmenter\u001b[39;00m(Augmenter):\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"An implementation of Easy Data Augmentation, which combines:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    - WordNet synonym replacement\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    https://arxiv.org/abs/1901.11196\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/textattack/constraints/pre_transformation/stopword_modification.py:21\u001b[0m, in \u001b[0;36mStopwordModification.__init__\u001b[0;34m(self, stopwords, language)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(language))\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m~/Downloads/deepL_assignment2/.venv/lib/python3.12/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/agathefernandesmachado/nltk_data'\n    - '/Users/agathefernandesmachado/Downloads/deepL_assignment2/.venv/nltk_data'\n    - '/Users/agathefernandesmachado/Downloads/deepL_assignment2/.venv/share/nltk_data'\n    - '/Users/agathefernandesmachado/Downloads/deepL_assignment2/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}],"source":["from textattack.augmentation import Augmenter\n","from textattack.transformations import WordSwapQWERTY\n","from textattack.transformations import WordSwapExtend\n","from textattack.transformations import WordSwapContract\n","from textattack.transformations import WordSwapHomoglyphSwap\n","from textattack.transformations import CompositeTransformation\n","from textattack.transformations import WordSwapRandomCharacterDeletion\n","from textattack.transformations import WordSwapNeighboringCharacterSwap\n","from textattack.transformations import WordSwapRandomCharacterInsertion\n","from textattack.transformations import WordSwapRandomCharacterSubstitution"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"WM-CP6fuSBvb"},"outputs":[{"ename":"NameError","evalue":"name 'WordSwapContract' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m augmentations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word_swap_contract:\n\u001b[0;32m---> 29\u001b[0m   augmentations\u001b[38;5;241m.\u001b[39mappend(\u001b[43mWordSwapContract\u001b[49m())\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word_swap_extend:\n\u001b[1;32m     31\u001b[0m   augmentations\u001b[38;5;241m.\u001b[39mappend(WordSwapExtend())\n","\u001b[0;31mNameError\u001b[0m: name 'WordSwapContract' is not defined"]}],"source":["# Word-level Augmentations\n","# This technique involves replacing a word with a contracted form of it, such as replacing \"cannot\" with \"can't\".\n","word_swap_contract = True\n","#This likely involves extending a word by adding extra characters, such as replacing \"good\" with \"goood\".\n","word_swap_extend = False\n","# Homoglyphs are characters that look visually similar but have different Unicode representations. This technique involves replacing characters in words with visually similar characters, such as replacing \"O\" with \"0\".\n","word_swap_homoglyph_swap = False\n","\n","\n","# Character-level Augmentations\n","# This technique swaps neighboring characters within words. For example, \"hello\" might become \"ehllo\".\n","word_swap_neighboring_character_swap = True\n","# This likely involves replacing characters with adjacent characters on a QWERTY keyboard layout. For instance, \"cat\" might become \"vat\".\n","word_swap_qwerty = False\n","# This technique randomly deletes characters from words. For example, \"house\" might become \"huse\".\n","word_swap_random_character_deletion = False\n","# This involves randomly inserting characters into words. For instance, \"book\" might become \"blook\"\n","word_swap_random_character_insertion = False\n","# This technique randomly substitutes characters in words with other characters. For example, \"apple\" might become \"apxle\".\n","word_swap_random_character_substitution = False\n","\n","# Check all the augmentations that you wish to apply!\n","\n","# NOTE: Try applying each augmentation individually, and observe the changes.\n","\n","# Apply augmentations\n","augmentations = []\n","if word_swap_contract:\n","  augmentations.append(WordSwapContract())\n","if word_swap_extend:\n","  augmentations.append(WordSwapExtend())\n","if word_swap_homoglyph_swap:\n","  augmentations.append(WordSwapHomoglyphSwap())\n","if word_swap_neighboring_character_swap:\n","  augmentations.append(WordSwapNeighboringCharacterSwap())\n","if word_swap_qwerty:\n","  augmentations.append(WordSwapQWERTY())\n","if word_swap_random_character_deletion:\n","  augmentations.append(WordSwapRandomCharacterDeletion())\n","if word_swap_random_character_insertion:\n","  augmentations.append(WordSwapRandomCharacterInsertion())\n","if word_swap_random_character_substitution:\n","  augmentations.append(WordSwapRandomCharacterSubstitution())\n","\n","transformation = CompositeTransformation(augmentations)\n","augmenter = Augmenter(transformation=transformation,\n","                      transformations_per_example=1)\n","\n","\n","review = \"I loved the food and the service was great!\"\n","augmented_review = augmenter.augment(review)[0]\n","print(\"Augmented review:\\n\")\n","print(augmented_review)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["import random"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"0vvByyrVSBvc"},"outputs":[],"source":["def getPrediction(text, model):\n","  \"\"\"\n","  Outputs model prediction based on the input text.\n","\n","  Args:\n","    text: String\n","      Input text\n","\n","  Returns:\n","    item of pred: Iterable\n","      Prediction on the input text\n","  \"\"\"\n","  inputs = tokenizer(text, padding=\"max_length\", max_length=256,\n","                     truncation=True, return_tensors=\"pt\")\n","  for key, value in inputs.items():\n","    inputs[key] = value.to(model.device)\n","\n","\n","  outputs = model(**inputs)\n","  logits = outputs.logits\n","  pred = torch.argmax(logits, dim=1)\n","  return pred.item()"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'word_swap_contract': True, 'word_swap_extend': True, 'word_swap_homoglyph_swap': False, 'word_swap_neighboring_character_swap': False, 'word_swap_qwerty': True, 'word_swap_random_character_deletion': True, 'word_swap_random_character_insertion': True, 'word_swap_random_character_substitution': True}, {'word_swap_contract': True, 'word_swap_extend': False, 'word_swap_homoglyph_swap': False, 'word_swap_neighboring_character_swap': True, 'word_swap_qwerty': True, 'word_swap_random_character_deletion': True, 'word_swap_random_character_insertion': False, 'word_swap_random_character_substitution': True}, {'word_swap_contract': True, 'word_swap_extend': True, 'word_swap_homoglyph_swap': True, 'word_swap_neighboring_character_swap': True, 'word_swap_qwerty': True, 'word_swap_random_character_deletion': True, 'word_swap_random_character_insertion': True, 'word_swap_random_character_substitution': True}, {'word_swap_contract': True, 'word_swap_extend': True, 'word_swap_homoglyph_swap': False, 'word_swap_neighboring_character_swap': True, 'word_swap_qwerty': True, 'word_swap_random_character_deletion': True, 'word_swap_random_character_insertion': True, 'word_swap_random_character_substitution': False}, {'word_swap_contract': True, 'word_swap_extend': True, 'word_swap_homoglyph_swap': False, 'word_swap_neighboring_character_swap': False, 'word_swap_qwerty': True, 'word_swap_random_character_deletion': True, 'word_swap_random_character_insertion': False, 'word_swap_random_character_substitution': False}, {'word_swap_contract': True, 'word_swap_extend': False, 'word_swap_homoglyph_swap': False, 'word_swap_neighboring_character_swap': False, 'word_swap_qwerty': True, 'word_swap_random_character_deletion': True, 'word_swap_random_character_insertion': False, 'word_swap_random_character_substitution': False}, {'word_swap_contract': True, 'word_swap_extend': True, 'word_swap_homoglyph_swap': True, 'word_swap_neighboring_character_swap': False, 'word_swap_qwerty': True, 'word_swap_random_character_deletion': True, 'word_swap_random_character_insertion': True, 'word_swap_random_character_substitution': True}, {'word_swap_contract': True, 'word_swap_extend': True, 'word_swap_homoglyph_swap': True, 'word_swap_neighboring_character_swap': False, 'word_swap_qwerty': True, 'word_swap_random_character_deletion': True, 'word_swap_random_character_insertion': True, 'word_swap_random_character_substitution': True}, {'word_swap_contract': True, 'word_swap_extend': True, 'word_swap_homoglyph_swap': False, 'word_swap_neighboring_character_swap': False, 'word_swap_qwerty': True, 'word_swap_random_character_deletion': True, 'word_swap_random_character_insertion': False, 'word_swap_random_character_substitution': True}, {'word_swap_contract': True, 'word_swap_extend': True, 'word_swap_homoglyph_swap': False, 'word_swap_neighboring_character_swap': True, 'word_swap_qwerty': True, 'word_swap_random_character_deletion': True, 'word_swap_random_character_insertion': False, 'word_swap_random_character_substitution': False}]\n"]}],"source":["models = [ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=True), \n","          ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=False, with_attn=False),\n","          ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=False, with_attn=True),\n","          ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=4, block='prenorm', dropout=0.3),\n","          ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='postnorm', dropout=0.3)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_samples_to_see = 1000\n","result_dict = {\n","    'configuration': [],\n","    'model': [],\n","    'f1_score': [],\n","    'accuracy': []\n","}\n","\n","for j in range(10):\n","  word_swap_contract = True\n","  word_swap_extend = random.choice([True, False])\n","  word_swap_homoglyph_swap = True\n","  word_swap_neighboring_character_swap = random.choice([True, False])\n","  word_swap_qwerty = random.choice([True, False])\n","  word_swap_random_character_deletion = random.choice([True, False])\n","  word_swap_random_character_insertion = True\n","  word_swap_random_character_substitution = random.choice([True, False])\n","  config = {\n","            'word_swap_contract': word_swap_contract,\n","            'word_swap_extend': word_swap_extend,\n","            'word_swap_homoglyph_swap': word_swap_homoglyph_swap,\n","            'word_swap_neighboring_character_swap': word_swap_neighboring_character_swap,\n","            'word_swap_qwerty': word_swap_qwerty,\n","            'word_swap_random_character_deletion': word_swap_random_character_deletion,\n","            'word_swap_random_character_insertion': word_swap_random_character_insertion,\n","            'word_swap_random_character_substitution': word_swap_random_character_substitution\n","        }\n","  augmentations = []\n","  if word_swap_contract:\n","    augmentations.append(WordSwapContract())\n","  if word_swap_extend:\n","    augmentations.append(WordSwapExtend())\n","  if word_swap_homoglyph_swap:\n","    augmentations.append(WordSwapHomoglyphSwap())\n","  if word_swap_neighboring_character_swap:\n","    augmentations.append(WordSwapNeighboringCharacterSwap())\n","  if word_swap_qwerty:\n","    augmentations.append(WordSwapQWERTY())\n","  if word_swap_random_character_deletion:\n","    augmentations.append(WordSwapRandomCharacterDeletion())\n","  if word_swap_random_character_insertion:\n","    augmentations.append(WordSwapRandomCharacterInsertion())\n","  if word_swap_random_character_substitution:\n","    augmentations.append(WordSwapRandomCharacterSubstitution())\n","\n","  transformation = CompositeTransformation(augmentations)\n","  augmenter = Augmenter(transformation=transformation,\n","                        transformations_per_example=1)\n","\n","  for model in models:\n","    pred_labels = []\n","    true_labels = []\n","    for i in range(n_samples_to_see):\n","      text = dataset_test[i][\"text\"]\n","      label = dataset_test[i][\"label\"]\n","      augmented_text = augmenter.augment(text)[0]\n","      pred_label = getPrediction(augmented_text, model)\n","      pred_labels.append(pred_label)\n","      true_labels.append(label)\n","    accuracy = accuracy_score(true_labels, pred_labels)\n","    f1 = f1_score(true_labels, pred_labels)\n","    result_dict['configuration'].append(config)\n","    result_dict['model'].append(model)\n","    result_dict['f1_score'].append(f1_score)\n","    result_dict['accuracy'].append(accuracy)\n","\n","save_logs(result_dict, \"assignment/aug\", 'aug')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}}},"nbformat":4,"nbformat_minor":0}
